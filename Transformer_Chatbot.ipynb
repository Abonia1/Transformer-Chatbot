{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2+ePpsNRXBFb8uDrFCfRS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abonia1/Transformer-Chatbot/blob/main/Transformer_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM4AlYlzaFoB",
        "outputId": "5dc44686-c3e9-4a4b-f8a0-9823324c07ba"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "assert tf.__version__.startswith('2')\r\n",
        "tf.random.set_seed(1234)\r\n",
        "\r\n",
        "!pip install tensorflow-datasets==1.2.0\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "\r\n",
        "import os\r\n",
        "import re\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-datasets==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/34/ff424223ed4331006aaa929efc8360b6459d427063dc59fc7b75d7e4bab3/tensorflow_datasets-1.2.0-py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==1.2.0) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==1.2.0) (0.16.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==1.2.0) (20.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==1.2.0) (5.4.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==1.2.0) (2.23.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==1.2.0) (0.27.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==1.2.0) (1.12.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==1.2.0) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==1.2.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==1.2.0) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==1.2.0) (0.3.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==1.2.0) (0.10.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==1.2.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets==1.2.0) (3.12.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (3.0.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets==1.2.0) (1.52.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets==1.2.0) (53.0.0)\n",
            "Installing collected packages: tensorflow-datasets\n",
            "  Found existing installation: tensorflow-datasets 4.0.1\n",
            "    Uninstalling tensorflow-datasets-4.0.1:\n",
            "      Successfully uninstalled tensorflow-datasets-4.0.1\n",
            "Successfully installed tensorflow-datasets-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eE5n_0taUTt",
        "outputId": "9bf37fec-d4b2-434c-ebbf-57aeff265ed5"
      },
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\r\n",
        "    'cornell_movie_dialogs.zip',\r\n",
        "    origin=\r\n",
        "    'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\r\n",
        "    extract=True)\r\n",
        "\r\n",
        "path_to_dataset = os.path.join(\r\n",
        "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\r\n",
        "\r\n",
        "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\r\n",
        "path_to_movie_conversations = os.path.join(path_to_dataset,\r\n",
        "                                           'movie_conversations.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "9920512/9916637 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcjoPtFsaVKI"
      },
      "source": [
        "# Maximum number of samples to preprocess\r\n",
        "MAX_SAMPLES = 50000\r\n",
        "\r\n",
        "def preprocess_sentence(sentence):\r\n",
        "  sentence = sentence.lower().strip()\r\n",
        "  # creating a space between a word and the punctuation following it\r\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\r\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\r\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\r\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\r\n",
        "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\r\n",
        "  sentence = sentence.strip()\r\n",
        "  # adding a start and an end token to the sentence\r\n",
        "  return sentence\r\n",
        "\r\n",
        "\r\n",
        "def load_conversations():\r\n",
        "  # dictionary of line id to text\r\n",
        "  id2line = {}\r\n",
        "  with open(path_to_movie_lines, errors='ignore') as file:\r\n",
        "    lines = file.readlines()\r\n",
        "  for line in lines:\r\n",
        "    parts = line.replace('\\n', '').split(' +++$+++ ')\r\n",
        "    id2line[parts[0]] = parts[4]\r\n",
        "\r\n",
        "  inputs, outputs = [], []\r\n",
        "  with open(path_to_movie_conversations, 'r') as file:\r\n",
        "    lines = file.readlines()\r\n",
        "  for line in lines:\r\n",
        "    parts = line.replace('\\n', '').split(' +++$+++ ')\r\n",
        "    # get conversation in a list of line ID\r\n",
        "    conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\r\n",
        "    for i in range(len(conversation) - 1):\r\n",
        "      inputs.append(preprocess_sentence(id2line[conversation[i]]))\r\n",
        "      outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\r\n",
        "      if len(inputs) >= MAX_SAMPLES:\r\n",
        "        return inputs, outputs\r\n",
        "  return inputs, outputs\r\n",
        "\r\n",
        "\r\n",
        "questions, answers = load_conversations()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol_vNwoYab04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea212193-eb6b-4655-aa6b-2aef6b8f7b52"
      },
      "source": [
        "print('Sample question: {}'.format(questions[25]))\r\n",
        "print('Sample answer: {}'.format(answers[25]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample question: you know chastity ?\n",
            "Sample answer: i believe we share an art instructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tbd6iMQqS_co"
      },
      "source": [
        "# Build tokenizer using tfds for both questions and answers\r\n",
        "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\r\n",
        "    questions + answers, target_vocab_size=2**13)\r\n",
        "\r\n",
        "# Define start and end token to indicate the start and end of a sentence\r\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\r\n",
        "\r\n",
        "# Vocabulary size plus start and end token\r\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0a81tOcTgYf",
        "outputId": "ed23e367-d9a6-4848-e3cb-10f8b1abbff1"
      },
      "source": [
        "print('Tokenized sample question: {}'.format(tokenizer.encode(questions[25])))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized sample question: [5, 50, 8006, 3583, 8196, 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGJNX7S6TkGd"
      },
      "source": [
        "# Maximum sentence length\r\n",
        "MAX_LENGTH = 40\r\n",
        "\r\n",
        "\r\n",
        "# Tokenize, filter and pad sentences\r\n",
        "def tokenize_and_filter(inputs, outputs):\r\n",
        "  tokenized_inputs, tokenized_outputs = [], []\r\n",
        "  \r\n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\r\n",
        "    # tokenize sentence\r\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\r\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\r\n",
        "    # check tokenized sentence max length\r\n",
        "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\r\n",
        "      tokenized_inputs.append(sentence1)\r\n",
        "      tokenized_outputs.append(sentence2)\r\n",
        "  \r\n",
        "  # pad tokenized sentences\r\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\r\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\r\n",
        "  \r\n",
        "  return tokenized_inputs, tokenized_outputs\r\n",
        "\r\n",
        "\r\n",
        "questions, answers = tokenize_and_filter(questions, answers)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxGheyud61mO",
        "outputId": "8469c7b6-b18c-4919-c8ab-182c5a545517"
      },
      "source": [
        "print('Vocab size: {}'.format(VOCAB_SIZE))\r\n",
        "print('Number of samples: {}'.format(len(questions)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 8333\n",
            "Number of samples: 44095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r239vR-S62d6"
      },
      "source": [
        "BATCH_SIZE = 64\r\n",
        "BUFFER_SIZE = 20000\r\n",
        "\r\n",
        "# decoder inputs use the previous target as input\r\n",
        "# remove START_TOKEN from targets\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\r\n",
        "    {\r\n",
        "        'inputs': questions,\r\n",
        "        'dec_inputs': answers[:, :-1]\r\n",
        "    },\r\n",
        "    {\r\n",
        "        'outputs': answers[:, 1:]\r\n",
        "    },\r\n",
        "))\r\n",
        "\r\n",
        "dataset = dataset.cache()\r\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\r\n",
        "dataset = dataset.batch(BATCH_SIZE)\r\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}